#Transformers.....¡¡ Ensamble ¡¡
for  along time in deep learnning image processing and recognition was the  hip new thing
because Cnns , but for languaje the best we had was Rnns, the thing is that languaje is secuantial, so if you 
shuffle the order of the woeds  on a sentence you change  the sentence meanning, so they were used for  translation
and did good  for short sentence but with long sentences,lets say the RNNs have a little les memory 
than Dory from Nemo. so 

https://www.youtube.com/watch?v=SZorAJ4I-sA


## positional encoding using sin waves

## self atention mechanism 
cosine similarity between embediongs in a recalculated space that has in  acount the context but 
i stil dont understand how they pass from a general space to a contextual space 
