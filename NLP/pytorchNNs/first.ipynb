{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a1bd0d5",
   "metadata": {},
   "source": [
    "#Core concepts pythorch\n",
    "## Tensors\n",
    "fundamental structure, like a  number py array but optimice for  math operations\n",
    "    - creation is done with :torch.tensor,torch.randn, between others\n",
    "    - you can move them .to(device) or .cuda() \n",
    "    - also convert them tu numpy arrays is posible with torch.from_numpy() and .numpy()\n",
    "in code readding how many dimentions a tensor have is same as how many square parentesis it has at the beguinning of the output when you print it\n",
    "### broadcasting\n",
    "Element wise operations are easy to understand,  if you have 2 tensors of lets say 2 dimentions each, and you want to do a  element wise sum you will summ elements on position 0,0  ;0,1; 1,1;1,0  of both tensors so the resuld will be a 2 dimention tensor. Now from this you can  infer  tensors most be same dimention right well not necesary, here is were broadcsting enters the scene.\n",
    "if one of your tensors is 1 dimension bigger than the other, torch will copy and paste the tensor (one with less dimensions )to fill up the other dimension that is missing.\n",
    "This concept actually  was born as  a new matematical notation to compress  complex matematical ideas\n",
    "#### data manipulation\n",
    "Other operations like slizing or filtering work pretty similar as you will write them in  pyton lists or pandas dataframes\n",
    "tensor[0,0:2]\n",
    "tensor[tensor<23]\n",
    "tensor.reshape(m,n)\n",
    "tensor.unsqueeze(4) # to add more dimensions\n",
    "tensor.squeeze(3) to destroy dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fabdc5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import torch.nn as nn\n",
    "class textClassifier :\n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_dim):\n",
    "        super(textClassifier,self).__init__()\n",
    "        self.embedding=nn.Embedding(vocab_size,embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        # se estan definiendo creo capas ocultas input o embedding layer y output layer basados en el template de las neural networks the pythorch\n",
    "        #lstm is a rnn that will act as hidden layer to manage secuentiality of text\n",
    "        def forward(self,text)        :\n",
    "            \"\"\"\"defines how the conections will be made inside the layers and how data flows\"\"\"\n",
    "            embeded=self.embedding(text)\n",
    "            outputs,(hidden,cell)=self.lstm(embeded)\n",
    "            final_output=self.fc(hidden[-1])\n",
    "            return final_output\n",
    "vocab_size = 1000  # Tamaño del vocabulario\n",
    "embedding_dim = 100  # Dimensiones del embedding\n",
    "hidden_dim = 256  # Dimensiones ocultas\n",
    "output_dim = 2  # Dimensiones de salida (clasificación binaria)\n",
    "\n",
    "model = TextClassifier(vocab_size, embedding_dim, hidden_dim, output_dim)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d094a129",
   "metadata": {},
   "source": [
    "## Auto grad\n",
    " gradients are ecentiall for back propagation and gradient descent like algorithms, so pytorch aoutomatically calculate them\n",
    "    - to track computations use .requires_grads= True\n",
    "    - to compute gradients use loss.backward()\n",
    "    - to desable tracking for eval or inference with torch.no_grad()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87838df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention (nn.Module):\n",
    "    \"\"\"this is the heart of transformmers architecture\"\"\"\n",
    "def __init__(delf, h,d_model):\n",
    "    \"\"\"inputs:\n",
    "            h # of heads\n",
    "            d_model dimentions of each head \"\"\"    \n",
    "    super(MultiHeadedAttention).__init__\n",
    "    self.h=h\n",
    "    self.d_model=d_model\n",
    "    self.linear_w0=nn.linear(dmodel,d_model)\n",
    "def forward (self,query,key,value):\n",
    "    query=query.view(-1,self.h,int(self.d_model/self.h)).transpose(1,2)\n",
    "    key=key.view(-1,self.h,int(self.d_model/self.h)).transpose(1,2)\n",
    "    value=value.vew(-1,self.h,int(self.d_model/self.h)).transpose(1,2)\n",
    "    z,_=attention(query,key,value)\n",
    "    z=z.transpose(1,2).contiguos().view(n_palabras,-1)\n",
    "    z_w0=linnear_w0(z)\n",
    "    return z_w0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a2233",
   "metadata": {},
   "source": [
    "## trainning loop and loss functions\n",
    "torch allows you to have more control over  the trainning loop\n",
    "1. forward pass : outputs= model(inputs)\n",
    "2. compute loss : loss =criterion (outputs,targets)\n",
    "3. backward pass : loss.backward()\n",
    "4. update parameters: optimizer.step()\n",
    "5. Reset gradients : optimizer.zero_grad()\n",
    "the nn module has a ton of usable loss function available to uselike\n",
    "nn.CrossEntropyLoss , nn.MSEloss, nn.BCEloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313c86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data creation\n",
    "import pytorch\n",
    "import matplotlib.pyplot as plt\n",
    "X=torch.arrange(0,1,0.025).unsqueeze(1)\n",
    "Y=2.5*X+1\n",
    "train_size= int(0.7*len(X))\n",
    "Xtrain,Ytrain=X[:train_size],Y[:train_size]\n",
    "Xtest,Ytest=X[train_size:],Y[train_size:]\n",
    "#definitionof plot functions\n",
    "def plot(trainData,testData):\n",
    "    plt.scatter(trainData[0].numpy(), trainData[1].numpy(), label='Entrenamiento', color='green')\n",
    "    plt.scatter(testData[0].numpy(), testData[1].numpy(), label='Prueba', color='yellow')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "#model class\n",
    "class linearRegModel:\n",
    "    def __init__(self):\n",
    "        self.volume= nn.Parameter(torch.randn(1,dtype=torch.float),requires_grad=True)\n",
    "        self.bias= nn.Parameter(torch.randn(1,dtype=torch.float),requires_grad=True)\n",
    "    def forward(self,x:torch.Tensor)->torch.Tensor:\n",
    "        return  self.volume * x+self.bias\n",
    "        \n",
    "#model init\n",
    "torch.manual_seeds(37)# setting random state in sklearn\n",
    "model=linearRegModel()\n",
    "model.stateDic()#givesme parameters as a dictionary\n",
    "#inferences\n",
    "with torch.inference_mode:\n",
    "    pred=model(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b7651b",
   "metadata": {},
   "source": [
    "## Data sets  and data loaders\n",
    "These are abstractions that allows  the handeling and processing of big data  for model development in a efficient way\n",
    "\n",
    "so the dataset class eather saves al the data in a instance or allowws you to acces the data in real time via methods of the class, the dataloader class recives as  a parameter a dataset instance of the class and alows you to shuffle it, separate it in batches, and create  multiple workers to process it in a paralel way.\n",
    "\n",
    "so for this to work the dataset class needs \n",
    "__len__() method that opens a comunication path between data loader and dataset where data loader shares how many samples are contain in the data set class\n",
    "\n",
    "__get_item__() method that recives the index of the data sample and retrives the data sample itself\n",
    "\n",
    "each worker having its own subset of indexes retrive the data and put it in a box (when creating the batch) and then  a collate functions convert this individual samples in to a  batched tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f7627b",
   "metadata": {},
   "source": [
    "### Optimizers\n",
    "learning rate by the gradient and then substrac the step  to the parameters\n",
    "##### stochastic GD\n",
    "it uses a single trinning example wich makes it have to make a lot of steps, and take a d tour before   arriving to a minimun\n",
    "##### mini batch GD\n",
    "uses small baches of data to upgrate the   gradient, so it takes less steps, but still we have the problem of the step becoming smaller as it gets to the minimun\n",
    "##### SGD with momentum\n",
    "it let the \"ball\" gain speed as it travels down hill making the  rate larger when its going in the right direction \n",
    "##### ADA grad\n",
    "some times momentum avershut the  minimun so ada fix this alowing a  adaptative learnning rate for each parameter.\n",
    "#### RMS prop\n",
    "it keeps the steps to become to small when the gradient is too small but still far from the minimun\n",
    "###### ADAM\n",
    "the most  used  in neural networks it takes the mean of the passed gradients so it hass momentumm  enoght to surpass local minimun it calculates a second moment  based on the rood squeare minimun\n",
    "\n",
    "all this can be found on the sgd module"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526583b4",
   "metadata": {},
   "source": [
    "### transfer learnning and fine tunnin\n",
    "\n",
    "you can load pre trainned models and  freeze some of its layers\n",
    "    for params in model.parameters():\n",
    "        params.requires_grad= false\n",
    "\n",
    "conecting it to a new layer for classification and train just the  new layer \n",
    "import torch.nn as nn\n",
    "model.fc = nn.Linear(model.fc.in_features, 5)  # e.g., 5 new classes\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "#### saving models\n",
    " you can sabe the weigths the optimizer state or the full artifact, usualy the weigths  and yo  load them in the same architecture. \n",
    " It's also possible to load intermidient trainning states.\n",
    "\n",
    "\n",
    " #### evaluation and inference  \n",
    " you can activate eval mode to desabel drop out, disable gradient tracking to save memory and do inference faster\n",
    " you can move predictions to cpu if you need it \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf510f8a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "040457d7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88f5d476",
   "metadata": {},
   "source": [
    "Would you like me to create a mini 7-day practice roadmap (with 20–30 min exercises each day) that takes you from tensors → custom model → fine-tuning a pretrained model in PyTorch?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd58ae4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLMs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
