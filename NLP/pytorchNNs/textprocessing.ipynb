{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9958a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-51876ca1-faa5-4594-8d4e-f7c856f427aa\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-51876ca1-faa5-4594-8d4e-f7c856f427aa\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3128307969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#if rnned in a colab remote server using ngrok\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install -r requirements.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     70\u001b[0m   \"\"\"\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    165\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    166\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#if runned in a colab remote server using ngrok\n",
    "from google.colab import files\n",
    "uploaded = files.upload() \n",
    "%pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357d19a4",
   "metadata": {},
   "source": [
    "## NLTK vs PyTorch\n",
    "\n",
    "**Tokenizers in NLTK** are regex- or heuristic-based, sometimes using statistics.  \n",
    "After tokenization, these are mapped to frequency counts, e.g., with a **TF-IDF vectorizer** or a manually created vocabulary dictionary (lexicon).  \n",
    "NLTK is **not optimized** for training neural networks.  \n",
    "\n",
    "**Tokenizers in PyTorch**, on the other hand, are **pretrained mappers** of character sequences into subword units.  \n",
    "They split the input text into those units and assign each one an ID.  \n",
    "\n",
    "A **vocab object** in PyTorch wraps all these attributes using dictionaries or hash maps.  \n",
    "Its output is a **tensor of integers**.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8627439f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-350031855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDBpedia\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'",
      "",
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import DBpedia\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "print(torchtext.__version__)\n",
    "\n",
    " #dbpedia has already a train split\n",
    "#the iter object  makes  each item of the training manually available\n",
    "#creating a vocab with the tokenize text\n",
    "tokenizer=get_tokenizer(\"basic_english\")\n",
    "train_iter = DBpedia(root=\".data\", split=\"train\")\n",
    "\n",
    "def yield_token(iter):\n",
    "    for _, texto in iter:\n",
    "        yield tokenizer(texto)#makes the function a generator one value  at a time on demand\n",
    "vocab= build_vocab_from_iterator(yield_token(iter),specials=[\"<unk>\"])\n",
    "vocab.set_default_index(vocab[\"<unk>\"]) #maps each token on the set to one number , inyective function like\n",
    "text_pipeline= lambda z: vocab(tokenizer(z))\n",
    "label_pipeline=lambda x: int(x)-1\n",
    "text_pipeline(\"esto sera convertido a tokens y luego cada token a un numero\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4477e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a881d5",
   "metadata": {},
   "source": [
    "Before neural embeddings became standard, text had to be processed and simplified so that methods like **word2vec** or **TF-IDF** could be applied.  \n",
    "\n",
    "Now, we define the function `collate`, which by default stacks whatever your `DataLoader` outputs into tensors using built-in functions, assuming all data has the same properties (like length).  \n",
    "\n",
    "In our case, instead of padding the sentences to make them all the same length, we keep track of their lengths in the **offsets** variable. This allows us to know where each sentence begins while preserving the order, so that the labels remain aligned with the offsets array.  \n",
    "\n",
    "---\n",
    "\n",
    "#### Pooling and EmbeddingBag  \n",
    "\n",
    "**Pooling** is the process of compressing many vectors into just one.  \n",
    "- **Max pooling**: take the largest feature value from each vector.  \n",
    "- **Mean pooling**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b624852a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'pytorch (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n pytorch ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # Q: Is it necessary always to define this function?\n",
    "    # A: No. Only when you need custom batching logic. \n",
    "    # For example, with text data of variable lengths, \n",
    "    # we need to pad/offset them properly before putting them in a single tensor.\n",
    "\n",
    "    label_list = []\n",
    "    text_list = []\n",
    "    offsets = [0]\n",
    "\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "\n",
    "        # Q: What are the labels of DBpedia and what are they used for?\n",
    "        # A: DBpedia is a dataset for text classification. \n",
    "        # Each \"label\" is a category (e.g., Company, Artist, Place, etc.).\n",
    "        # They are the target classes for supervised learning.\n",
    "\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    # we compute cumulative sum of text lengths so we know where each new text starts\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "\n",
    "    # Q: Why is it important to send this to the device again?\n",
    "    # A: Because DataLoader may create tensors on CPU by default. \n",
    "    # Sending to device ensures data is on GPU (if available) before the model uses it.\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "\n",
    "# \"iter\" should be replaced with your actual dataset (e.g., train_dataset)\n",
    "# Example:\n",
    "# dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_batch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae13c26c",
   "metadata": {},
   "source": [
    "now we will create  the architecture, using functional as F that includes the fundamentals building blocks  of  all other modules on nn module.\n",
    "\n",
    "had in my mind the idea that when a vector was inputed into a sigmoid function the vector will be normalize between -1 and 1, but you are tellingme thats not what the batch norm layer is doing but using conventional normalization metods la z normalization to mormalize all vectors. in a batch or a data set?\n",
    "\n",
    "Yes and it does it using a mini batch not the entire data set. also the sigmoids outputs go from (0 to 1) its use at the end to map to probabilitys the output.\n",
    "\n",
    "see the big diference between the two is  sigmoid compresses, z - normalization  rescales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dabd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModelClasificationText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_classes):\n",
    "        # A: This initializes the parent class (nn.Module). \n",
    "        # It sets up all the internal machinery that PyTorch needs \n",
    "        # to register layers, track parameters, and build the computation graph.\n",
    "        super(ModelClasificationText, self).__init__()\n",
    "\n",
    "        # A: nn.EmbeddingBag creates a lookup table for vocab_size words,\n",
    "        # each represented by a vector of size embed_dim.\n",
    "        # When you pass text+offsets, it fetches embeddings for the tokens\n",
    "        # and pools them (sum, mean, or max).\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, mode=\"mean\")\n",
    "\n",
    "        # Batch Normalization layer\n",
    "        # This is NOT a sigmoid function. It's a normalization step that\n",
    "        # helps training converge faster and reduces internal covariate shift.\n",
    "        self.bn1 = nn.BatchNorm1d(embed_dim)\n",
    "\n",
    "        # Fully connected layer\n",
    "        # Q: What does this do?\n",
    "        # A: It maps the normalized + activated embedding vector (size embed_dim)\n",
    "        # into the number of classes (num_classes) you want to predict.\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        # Apply embedding bag: turns tokens into pooled sentence embedding\n",
    "        embedded = self.embedding(text, offsets)\n",
    "\n",
    "        # Normalize features\n",
    "        normal = self.bn1(embedded)\n",
    "\n",
    "        # Non-linear activation\n",
    "        # Q: Why use ReLU here?\n",
    "        # A: ReLU introduces non-linearity. Without it, the model would be \n",
    "        # equivalent to a single linear transformation, limiting its capacity.\n",
    "        # ReLU prevents collapsing to a simple linear mapping and lets the\n",
    "        # network learn more complex decision boundaries.\n",
    "        activated = F.relu(normal)\n",
    "\n",
    "        # Map to class scores ( soft max's logits)\n",
    "        return self.fc(activated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f225689",
   "metadata": {},
   "source": [
    "for a shallow model relu, if dead neurons appear, try leakky relu, if data is simetric around zero, tanh could be good, if the net is too deep gelu is your activation function, for the output stick to :\n",
    "sigmoid, softmax or none for  binary,multiclas clasification or  for regression.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bdf2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_categories = len(set([label for label, text in iter]))  # 'iter' must be defined, likely your dataset/iterator\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 100  # you asked about this below\n",
    "model = ModelClasificationText(vocab_size=vocab_size, embed_dim=embedding_size, num_classes=num_categories)\n",
    "print(model)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "def train(dataloader, epoch):  # you need epoch as parameter for the print at the end\n",
    "    model.train()  # put the model in training mode\n",
    "    epoch_acc = 0\n",
    "    epoch_loss = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()  # reset gradients from last batch\n",
    "        prediction = model(text, offsets)\n",
    "        loss = critiria(prediction, label)  # typo: 'critiria' should be 'criterion'\n",
    "\n",
    "        # backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        acc = (prediction.argmax(1) == label).sum()\n",
    "        # argmax returns the class index with highest probability for each row\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)  # prevent exploding gradients\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "        if idx % 500 == 0 and idx > 0:\n",
    "            print(f\"epoch {epoch} | {idx/len(dataloader):.2f} batches | \"\n",
    "                  f\"loss {epoch_loss/total_count:.4f} | accuracy {epoch_acc/total_count:.4f}\")\n",
    "    return epoch_acc/total_count,epoch_loss/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6203d0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS= 3\n",
    "LEARNNING_RATE= 0.2\n",
    "BATCH_SIZE=64\n",
    "### tehis functions are also hiper parameters \n",
    "criteria=torch.nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.ADAM(model.parameters(),lr=LEARNNING_RATE)\n",
    "\n",
    "\n",
    "\n",
    "def eval(dataloader):\n",
    "    model.eval()\n",
    "    epoch_acc=0\n",
    "    total_count=0\n",
    "    epoch_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for idx,(label,text,offsets) in enumerate(dataloader):\n",
    "            prediction= model(text, offsets)\n",
    "            loss=criterion(prediction,label)\n",
    "            acc=(prediction.argmax(1)==label).sum()\n",
    "            epoch_loss +=loss.item()\n",
    "            epoch_acc+= acc.item()\n",
    "            total_count += label.size()\n",
    "    return epoch_acc/total_count,epoch_loss/total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5697314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DATA SPLIT\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "train,test=DBpedia()\n",
    "train_dataset=to_map_style_dataset(train)\n",
    "test_dataset= to_map_style_dataset(test)\n",
    "train_len=int(len(train_dataset)*0.95)\n",
    "train_split,val_split=random_split(train_dataset,[train_len,len(train_dataset)-train_len])\n",
    "train_dataloader=DataLoader(train_split,batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
    "val_dataloader=DataLoader(val_split,batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
    "test_dataloader=DataLoader(test,batch_size=BATCH_SIZE, shuffle=True,collate_fn=collate_batch)\n",
    "\n",
    "\n",
    "#now  we are ready to train\n",
    "\n",
    "major_loss_val=float(inf)\n",
    "for epoch in range (1,EPOCHS+1):\n",
    "\ttrain_acc,train_los =\ttrain(train_dataloader)\n",
    "\tval_acc,val_loss =\teval(val_dataloader)\n",
    "\tif val_loss< major_loss_validation:\n",
    "\t\tbest_valid_loss= val_loss\n",
    "\t\ttorch.save(model.state_dict(),\"best_saved.pt\")\n",
    "# why does it uses the  smalless loss of the val set inste of other set loss, is every epoc  defining a model and \n",
    "# the model of next epoch  is being a fine tune model of the previus epoch?\n",
    "#can you explain the process of trainning validation and testing?\n",
    "#now the test dataset\n",
    "#remember in eval function the model was set to eval mode wich made it not to actualize parameters?\n",
    "test_acc,test_loss=eval(test_dataloader)\n",
    "print(f\"Accurate del dataset -> {test_acc}\")\n",
    "print(f\"perdida del test dataset -> {test_loss}\")\n",
    "#now inference\n",
    "\n",
    "#here create a dict with the labels and the names of each label  DBpedia_labels\n",
    "\n",
    "def predict (text, text_pipeline):\n",
    "\twith torch.no_grad():\n",
    "\t\ttext=torch.tensor(text_pipeline(text))\n",
    "\t\topt_mode= torch.compile(model,mode=\"redice-overhead\")\n",
    "eg1=\" dont think sorry is easyly said, dont try turnning tabels  insted, you have take lots of chances befor, but aint  gonna give you any mor,thats how it goes, because part of me knows what you are thinking\"\n",
    "eg2=\" Don't say words you are going to regred,don't let the fire rush to your head, i have heard thouse accusations befor, and i'm not gonna take any more,bilive me, the sun in your eyes made some of your lies worth beliving\"\n",
    "eg3=\"don't let false ilussions behind, don't cry i ain't changing my mind, so find another fool, like before, because i am not going to keep anymor, bealinving, some of your lies, while all of the signs are deciving\"\n",
    "\n",
    "model= model.to('cpu') #if it was on gpu why its been send to cpu \n",
    "print(f\"the example if from category{DBpedia_label[predict(eg1)]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a526e18b",
   "metadata": {},
   "source": [
    "now storage and loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_state=model.state_dict()\n",
    "optimizer_state=optimizer.state_dict()\n",
    "checkpoint={\n",
    "    \"model_state\":model_state,\n",
    "    \"optimizer_state\":optimizer_state,\n",
    "    \"epoch\":EPOCHS,\n",
    "    \"loss\":train_loss,\n",
    "}\n",
    "torch.save(checkpoint,\"model_checkpoint.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ca3df5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
